\documentclass[12pt]{report}
\usepackage{consumer_resource_final}
\begin{document}
\subsection{Why do we solve it this way}\label{sec: explanation solve for d and m}
\textbf{TO DO : write this}
\subsection{Algorithmic procedure}\label{sec : feasibility methods algorithmic procedure}
We hereby detail the $\mathcal{A}$ procedure which from a set of metaparameters $m$ and a consumption-syntrophy network $(G,A)$ gives rise to a set of parameters $p$. It goes like this:
\begin{enumerate}
  \item We first draw randomly each $R^*_\nu$
  and $S^*_i$ :
  \begin{equation}
     R^*_\nu = \mathcal{R} \ \forall \nu=1, \dots, N_R\text{ and }  S^*_i = \mathcal{S} \ \forall i=1, \dots, N_S,
  \end{equation}
  where $\mathcal{R}$ and $\mathcal{S}$ are random variables coming from a distribution of mean equal to the corresponding metaparameter and relative standard deviation\footnote{By relative standard deviation, we mean the standard deviation measured in units of the average value.} $\epsilon$. In our simulations, we chose uniform distributions :
  \begin{equation}
  \mathcal{R} \sim \text{Unif}(R_0, R_0 \epsilon) \text{ and } \mathcal{S} \sim \text{Unif}(S_0, S_0\epsilon).
  \end{equation}
  \item The efficiency matrix $\sigma_{i\nu}$ is then drawn similarly, from a distribution with average $\sigma_0$. In order to simplify the problem\footnote{Indeed, we observerd that in general introducing a non uniform $\sigma_0$ adds an additional not needed layer of complexity.}, we take a zero-variance like \citeauthor{butler_stability_2018} in \cite{butler_stability_2018}, \ie all species consume resources at the same global efficiency :
  \begin{equation}
    \sigma_{i\nu} = \sigma_0.
  \end{equation}
  \item We build the consumption matrix $\gamma_{i\nu}$. Its adjacency matrix $G$ is loaded through a user-provided file.
  %$G$ is a binary matrix given by the user (in the \code{configuration.in} file) and is defined as the adjacency matrix of the consumption network (\ie it tells which species eats which resource). We then build $\gamma$ with the same network structure as $F$ (\ie both matrices have the same zero elements).
  While $G$ gives the structure of $\gamma$, \ie if a given $\gamma_{i\nu}$ is zero or not, the actual values of $\gamma_{i\nu}$ need then to be determined. They are drawn from a uniform distribution of mean $\gamma_0$ and relative standard deviation $\epsilon$:
  \begin{equation}
    \gamma_{i\nu} = \text{Unif}(\gamma_0, \gamma_0\epsilon) G_{i\nu}.
  \end{equation}
  \item We draw the resources external feeding rates, similarly to the other parameters :
  \begin{equation}
  l_\mu = \text{Unif}(l_0, l_0\epsilon) \ \forall \mu=1, \dots, N_R.
  \end{equation}
  \item The last free parameters are the non-zero elements of the syntrophy matrix $\alpha_{\nu i}$, since the $d_i$ and $l_\mu$ are determined through the equations of evolution at equilibrium. %This is the tricky part of the algorithm because $\alpha$ has to follow three constraints, namely energy conservation/dissipation Eq.\eqref{eq : dissipation constraint} and positiveness of $d_i$ and $l_\mu$ [\textbf{insert reference to equation}]. The general strategy is to choose the metaparameters in a way that these constraints should \textit{almost always} be satisfied, \ie we pick metaparameters that follow the feasibility constraint Eq.\eqref{eq : overall feasibility constraint metaparameters}.
  The adjacency matrix $A$ of $\alpha$ needs then to be specified. At the moment, it can be chosen in four different ways : fully connected, in a way that no resource eaten by a given species can be released by that same species (\ie no intraspecific syntrophy : $G_{i\mu}>0 \iff A_{\mu i}=0$), random structure with the same number of links as the consumption matrix and finally by a user provided matrix. After the adjacency matrix is loaded, we build $\alpha$ from a uniform distribution of mean $\alpha_0$ and relative standard deviation $\epsilon$:
  \begin{equation}
    \alpha_{\nu i} = \text{Unif}(\alpha_0, \alpha_0\epsilon) A_{\nu i} .
  \end{equation}
  %\item We build $\tau_{\nu i}$. It usually is equal to $\alpha_{\nu i}$ or 0.
  \item With all of these parameters drawn, we can solve Eqs.\eqref{eq : feasibility positive d} for the species death rate $d_i$ and Eqs.\eqref{eq : feasibility positive m} for $m_\nu$. All the parameters of the model are now fully determined.
  \item We check if the constraints Eqs.\eqref{eq : feasibility positive d and m} and \eqref{eq : feasability biomass conservation} on the parameters are fulfilled. If they are not, we go back to step 1. Otherwise, we can exit the algorithm, a feasible system has been built. We see here the advantage of having input metaparameters that will most likely give rise to a feasible parameter set. If we just give random metaparameters, we run the risk of getting stuck in an unpredictably long loop between steps 1 and 7. If however we are in a region where we know the metaparameters feasibility is high, a feasible system is found much faster.
\end{enumerate}

\subsection{When is zero part of the spectrum of $J^*$?}\label{sec : zero part of spectrum}
We are interested in knowing when $\lambda=0$ is part of the spectrum of $J^*$. By definition, $\lambda=0$ is an eigenvalue if and only if it solves the master equation \eqref{eq: master equation eigenvalues jacobian}:
\begin{equation}
\det
\begin{pmatrix}
  -\Delta   & \Gamma \\
  \Beta & 0
\end{pmatrix} = 0 \label{eq: determinant marginally stable equilibria}
\end{equation}
Using the fact that $\Delta$ is invertible, we can make use of the equality\footnote{This uses a formula which is trivially analogous to one found in \cite{powell_calculating_2011}.}:
\begin{equation}
\det\begin{pmatrix}
  -\Delta   & \Gamma \\
  \Beta & 0
\end{pmatrix} = \det(-\Delta)\det(\Beta\Delta^{-1}\Gamma).
\end{equation}
Eq.\eqref{eq: determinant marginally stable equilibria} then becomes:
\begin{equation}
\det(\Beta\Delta^{-1}\Gamma)=0
\end{equation}
which means that $\Beta \Delta^{-1}\Gamma$ is not full rank. Finally,
\begin{equation}
\boxed{
0 \in \sigma(J^*) \iff \Beta \Delta^{-1}\Gamma \text{ is not full rank.}\label{eq : condition zero in spectrum of J}
}
\end{equation}
But when is $\Beta \Delta^{-1}\Gamma$ not full rank? Sylvester rank inequality \cite{thome_inequalities_2016} states that:
\begin{equation}
\text{rank}(\Beta\Delta^{-1}\Gamma) \geq \text{rank}(\Beta)+\text{rank}(\Delta^{-1}\Gamma)
-N_R.
\end{equation}
Similarly,
\begin{equation}
\text{rank}(\Delta^{-1}\Gamma) \geq \text{rank}(\Delta^{-1})+\text{rank}(\Gamma)-N_R=\text{rank}(\Gamma),
\end{equation}
where we used the fact that $\Delta^{-1}$ is invertible so $\text{rank}(\Delta^{-1})=N_R$.
One of the standard rank properties is:
\begin{equation}
\text{rank}(\Delta^{-1}\Gamma) \leq \min\left\{\text{rank}(\Delta^{-1}), \text{rank}(\Gamma)\right\} \implies \text{rank}(\Delta^{-1}\Gamma) \leq \text{rank}(\Gamma).
\end{equation}
In the end this yields $\text{rank}(\Delta^{-1}\Gamma)=\text{rank}(\Gamma)$ and :
\begin{equation}
\text{rank}(\Beta\Delta^{-1}\Gamma) +N_R \geq \text{rank}(\Beta)+\text{rank}(\Gamma). \label{eq : lower bound Beta Delta Gamma}
\end{equation}
We can use this inequality to enunciate a lemma about the presence of zero in the spectrum of $J^*$.
\lemma{If $N_R \leq N_S$ and $\Beta$ and $\Gamma$ are full rank, then $0 \notin \sigma(J^*)$.}
\begin{proof}
{We assume that $0$ is in the spectrum of $J^*$ and prove it leads to a contradiction. Because $0 \in \sigma(J^*)$, Eq.\eqref{eq : condition zero in spectrum of J} implies $\Beta\Delta^{-1}\Gamma$ is not full rank. Since the largest possible value for the rank of matrix is the minimum between its number of rows and columns, we have :
\begin{equation}
\text{rank}(\Beta\Delta^{-1}\Gamma)< \min(N_R, N_S) = N_R.
\end{equation}
This can be used as an upper bound for Eq.\eqref{eq : lower bound Beta Delta Gamma} :
\begin{equation}
2 N_R > \text{rank}(\Beta)+\text{rank}(\Gamma).
\end{equation}
However, we also know that $\Beta$ and $\Gamma$ are full rank, \ie
\begin{equation}
\text{rank}(\Beta)=\text{rank}(\Gamma)=N_R.
\end{equation}
Hence the previous inequality amounts to $N_R > N_R$, which is a contradiction. We conclude that the hypothesis $0Â \in \sigma(J^*)$ is wrong.
}
\end{proof}

\subsection{Weak LRI regime}\label{sec : weak LRI regime}
\begin{theorem}
Let $p$ be a parameter set with a jacobian at equilibrium $J^*$. If $0$ is not an eigenvalue of $J^*$ and the equations
\begin{equation}
\left(\Gamma \Beta\right)_{\mu\mu} < - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}} \ \forall \mu,
\end{equation}
are verified, then the real eigenvalues of $J^*$ are negative.
\end{theorem}

\begin{proof}
We assume
\begin{equation}
\left(\Gamma \Beta\right)_{\mu\mu} < - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}}  \ \forall \mu.
\end{equation}
Let $\lambda \in \mathbb{R}$, then the following will also trivially hold:
\begin{equation}
  \left(\Gamma\Beta \right)_{\mu\mu} -\lambda^2 < - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}} \ \forall \mu. \label{eq: low species bound 1}
\end{equation}
Dividing Eq.\eqref{eq: low species bound 1} by $\Delta_\mu$, we get:
\begin{equation}
\frac{1}{\Delta_\mu}\left[\left(\sum_i \Gamma_{\mu i} \Beta_{i \mu} \right)-{\lambda^2}\right] < - \sum_{\nu \neq \mu } \abs{\frac{\sum_i \Gamma_{\mu i}\Beta_{i\nu}}{\Delta_\mu}} \ \forall \mu.
\end{equation}
Looking at Eq.\eqref{eq: definition S component wise}, we see that this is equivalent to:
\begin{equation}
S_{\mu \mu} + \sum_{\nu \neq \mu} \abs{S_{\mu \nu}} < 0\ \forall \mu.
\end{equation}
Using Lemma \ref{lemma: lemma Gerschgorin circle}, we know that all the real eigenvalues of $S(\lambda)$ will have a negative real part.
We can conclude with the statement of the theorem.
\end{proof}


\subsection{Effective system}
\normalfont
Models which involve the dynamics of species only are in general better known than consumers-resources models [\textbf{insert reference}]. In particular, a huge body of literature exists on the study of Lotka-Volterra systems [\textbf{insert reference}]. We may profit from this knowledge by transforming the effect of the resources dynamics into an effective consumers-only system.

This can be done by assuming that the resources reach an equilibrium way faster than the consumers.
Mathematically, that is equivalent to
\begin{equation}
  \frac{dR_\mu}{dt} \approx 0, \\ \forall \mu.
\end{equation}
Using Eq.\eqref{eq: differential eq for resources}, we get an explicit value for the resources:
\begin{equation}
  R_\mu \approx \frac{l_\mu+\sum_j \alpha_{\mu j}S_j}{m_\mu + \sum_k \gamma_{k\mu}S_k}.
\end{equation}
This expression can be used in Eq.\eqref{eq: differential eq for species} to get an effective system which describes the dynamics of the $N_S$ consumers:
\begin{equation}
  \frac{dS_i}{dt} = \left(\sum_\nu \left(\frac{\sigma_{i\nu}\gamma_{i\nu}l_\nu}{m_\nu+\sum_k \gamma_{k\nu}S_k} - \alpha_{\nu i}\right) -d_i + \sum_{\nu j} \frac{\sigma_{i\nu}\gamma_{i\nu}\alpha_{\nu j}}{m_\nu+\sum_{k}\gamma_{k\nu}S_k}S_j \right) S_i.
\end{equation}
This can be rewritten in a more compact way:
\begin{equation}
  \frac{dS_i}{dt} = p_i(S) S_i + \sum_j M_{ij}(S)S_i S_j \label{eq: effective equations of evolution}
\end{equation}
with
\begin{equation}
    p_i(S) = -\left(d_i+\sum_{\nu}\alpha_{\nu i}\right) + \sum_\nu \frac{\sigma_{i\nu}\gamma_{i\nu}l_\nu}{m_\nu+\sum_k \gamma_{k\nu}S_k}\text{ and } M_{ij}(S)=\sum_{\nu}\frac{\sigma_{i\nu}\gamma_{i\nu}\alpha_{\nu j}}{m_\nu+\sum_{k}\gamma_{k\nu}S_k}.
\end{equation}
If we assume the species $S_k$ are not too far away from their equilibrium values\footnote{Note that this is very rarely true, especially in the context of the study of structural stability, where entire species sometimes die out.}, \ie
\begin{equation}
S_k \approx S^*_k \ \forall k,
\end{equation}
then using Eq.\eqref{eq: positive m_nu} we can simplify $p_i$. Indeed,
\begin{equation}
m_\nu + \sum_k \gamma_{k\nu} S_k \approx m_\nu + \sum_k \gamma_{k\nu}S^*_k = \frac{l_\nu + \sum_k \alpha_{\nu k}S^*_k}{R^*_\nu} \label{eq: equality fluxes resource}
\end{equation}
Hence, the explicit dynamical dependence on $S$ can be removed from $p_i$ and $M_{ij}$:
\begin{equation}
p_i(S) \approx p_i \equiv - \left(d_i + \sum_\nu \alpha_{\nu i}\right) + \sum_\nu \frac{\sigma_{i\nu}\gamma_{i\nu}l_\nu R^*_\nu}{l_\nu + \sum_k \alpha_{\nu k}S^*_k},
\end{equation} and
\begin{equation}
M_{ij}(S) \approx M_{ij} \equiv \sum_\nu \frac{\sigma_{i\nu} \gamma_{i\nu} R^*_\nu \alpha_{\nu j}}{l_\nu + \sum_k{\alpha_{\nu k} S^*_k}}.
\end{equation}
\subsubsection{Perturbation analysis}
We study a system that we put close to an equilibrium $S^*$, \ie
\begin{equation}
S=S^*+\Delta S, \\ \text{with } \Delta S \ll 1.
\end{equation}
Written this way, the effective equations of motion Eq.\eqref{eq: effective equations of evolution} are equivalent to:
\begin{equation}
\frac{d\Delta S_i}{dt} = p_i(S^*+\Delta S)\left(S^*_i + \Delta S_i\right)+\sum_j M_{ij}(S^*+\Delta S)\left(S^*_i +\Delta S_i\right)\left(S^*_j +\Delta S_j\right).
\end{equation}
Since the deviations from equilibrium $\Delta S_i \ll 1$, we can forget the terms in higher power than quadratic:
\begin{equation}
\frac{d\Delta S_i}{dt} = \tilde{p}_i \Delta S_i + \sum_j E_{ij} \Delta S_j + \bigO(\Delta S^2),\label{eq: effective equ evol at O(D2)}
\end{equation}
with
\begin{equation}
\tilde{p}_i \equiv p_i(S^*) + \sum_k M_{ik}(S^*)S_k^*, \label{eq: tilde p effective system}
\end{equation}
and
\begin{equation}
E_{ij} \equiv \left(\partiald{p_i}{S_j}\evaluatedat{S^*}+M_{ij}(S^*)+\sum_k \partiald{M_{ik}}{S_j}\evaluatedat{S^*}S^*_k\right)S^*_i.
\end{equation}
After some computations, we can get $\tilde{p}_i$ and $E_{ij}$ in terms of the initial parameters. Indeed,
\begin{equation}
p_i(S^*)= -\left(d_i + \sum_\nu \alpha_{\nu i}\right) + \sum_\nu \frac{\sigma_{i\nu}\gamma_{i\nu}l_\nu}{m_\nu + \sum_k \gamma_{k\nu}S^*_k}
\end{equation}
and
\begin{equation}
M_{ik}(S^*) = \sum_\nu \frac{\sigma_{i\nu}\gamma_{i\nu}\alpha_{\nu j}}{m_\nu + \sum_k \gamma_{k\nu}S^*_k}.
\end{equation}
Hence, using Eq.\eqref{eq: tilde p effective system}:
\begin{equation}
\tilde{p}_i = - \left(d_i + \sum_\nu \alpha_{\nu i}\right) + \sum_\nu \frac{\sigma_{i\nu}\gamma_{i\nu}}{m_\nu + \sum_k \gamma_{k\nu}S^*_k}\left(l_\nu+\sum_{j}\alpha_{\nu j} S^*_j\right).
\end{equation}
This can be simplified using Eq.\eqref{eq: equality fluxes resource} and Eq.\eqref{eq: equilibrium species}:
\begin{equation}
\tilde{p}_i=-d_i +\sum_\nu \sigma_{i\nu}\gamma_{i\nu}R^*_\nu = \sum_\nu \alpha_{\nu i}.
\end{equation}
With a similar computation, one finds
\begin{equation}
E_{ij}=\sum_\nu \frac{\sigma_{i\nu}\gamma_{i\nu}S^*_i}{m_\nu+\sum_k \gamma_{k\nu}S^*_k} \left(\alpha_{\nu j}-\gamma_{j\nu}R^*_\nu\right).
\end{equation}
Finally, Eq.\eqref{eq: effective equ evol at O(D2)} can be recast in
\begin{equation}
\frac{d\Delta S_i}{dt} = \sum_j (J_E)_{ij} \Delta S_j,
\end{equation}
where the effective $N_S\times N_S$ jacobian matrix $J_E$ is defined by:
\begin{equation}
(J_E)_{ij}=\sum_\nu \left[\frac{\sigma_{i\nu}\gamma_{i\nu}S^*_i}{m_\nu+\sum_k \gamma_{k\nu}S^*_k} \left(\alpha_{\nu j}-\gamma_{j\nu}R^*_\nu\right)+\alpha_{\nu i}\delta_{ij}\right].
\end{equation}
We see that we without surprise we find again the $\Beta, \Gamma $ and $\Delta$ matrices coming from the jacobian at equilibrium:
\begin{equation}
\left(J_E\right)_{ij}=\sum_\nu \left[\frac{\Beta_{i\nu}\Gamma_{\nu j}}{\Delta_\nu}+\alpha_{\nu i} \delta_{ij}\right]
\end{equation}

This matrix determines the stability of the equilibrium. Namely if the largest eigenvalue of $J_E$ is positive, the equilibrium is unstable. If it is negative, the equilibrium is stable. If it is zero, the equilibrium is marginal.
\subsection{Measuring the feasibility and local dynamical stability volumes}\label{app: how to measure volume}
In the main text, we study how the sizes of both the fully feasible and the fully locally dynamically stable regions, respectively $\mathcal{F}_1^{G,A}(\alpha_0)$ and $\mathcal{D}_{L,1}^{G,A}(\alpha_0)$, change as a function of $G$, $A$ and $\alpha_0$. It is important to explain in what way (and why in \textit{that} way) these are computed. We define the \important{volume} of $\mathcal{F}^{G,A}_1(\alpha_0)$, or \define{feasibility volume}, as:
\begin{equation}
\vol{\mathcal{F}^{G,A}_1(\alpha_0)}\defined \frac{\iint_{(\gamma_0, S_0)\in[0,1]^2} d\gamma_0 \ dS_0 \ \mathcal{F}\left(m, G, A\right)}{\iint_{(\gamma_0, S_0)\in[0,1]^2} d\gamma_0 \ dS_0} = \iint_{(\gamma_0, S_0)\in[0,1]^2} d\gamma_0 \ dS_0 \ \mathcal{F}\left((\gamma_0, S_0, \alpha_0), G, A\right). \label{eq : app definition fully feasible volume of G, A}
\end{equation}
The \important{volume} of $\mathcal{D}_{L,1}^{G,A}(\alpha_0)$, or \define{(local) dynamical stability volume} is similarly defined:
\begin{equation}
\vol{\mathcal{D}_{L,1}^{G,A}(\alpha_0)}\defined \iint_{(\gamma_0, S_0)\in[0,1]^2} d\gamma_0 \ dS_0 \ \mathcal{D}_L\left((\gamma_0, S_0, \alpha_0), G, A\right).
\end{equation}
The feasibility and dynamical stability volumes do not \textit{stricto sensu} measure the area occupied by the fully feasible and fully dynamically stable regions, respectively. In the case of the feasibility volume, it is documented in the main text that the zone where $\mathcal{F}(m, G,A)$ is different from $1$ or $0$ is very small so measuring the volume this way does not provide a significant difference to naively counting the number of points for which the feasibility function is \textit{exactly} 1.

However, in the case of the dynamical stability volume, this approach would provide unsatisfactory results. Indeed, $\mathcal{D}_{L}(m, G, A)$ is a function that may be very patchy \textbf{TO DO: insert ref to picture of $D_L$ as a function of alpha0}: many points are not fully dynamically stable but rather \important{almost} fully dynamically stable, in the sense that there are many $m$ such that $\mathcal{D}_{L}(m, G, A)$ is extremely close to but not exactly equal to 1. Because of this, counting exactly the points where $\mathcal{D}_{L}(m, G, A)=1$ is very prone to noise, in the sense that it depends a lot on the precision at which $\mathcal{D}_L(m,G,A)$ is evaluated. We know that a lot of points are only \textit{almost} fully dynamically stable; if we increase the number of simulations to evaluate $\mathcal{D}_{L}(m, G, A)$, who can tell if the points that were previously identified as fully dynamically stable would be now only almost fully dynamically stable and hence not counted in the volume anymore?

Because of such considerations taking into account every point but pondering it by its value of the feasibility/dynamical stability function not only provides a measure close to the idea of measuring the ``full volume'' but also provides smoother and more robust results.

Following that line of thought, we also define the \important{common feasibility volume}, which is a measure of the common fully feasible region $\mathcal{F}_1^{S_M}(\alpha_0)$ of a matrix set $S_M$ at a given syntrophic interaction strength $\alpha_0$:
\begin{equation}
\vol{\mathcal{F}_1^{S_M}} \defined \iint_{(\gamma_0, S_0) \in [0,1]^2} d\gamma_0 \ dS_0 \min_{(G,A)\in S_M} \left\{\mathcal{F}\left((\gamma_0, S_0, \alpha_0), G, A\right)\right\}.
\end{equation}
In a completely analogous manner, the \important{common (local) dynamical stability volume} of a matrix set $S_M$ is given by:
\begin{equation}
\vol{\mathcal{D}_{L,1}^{S_M}} \defined \iint_{(\gamma_0, S_0) \in [0,1]^2} d\gamma_0 \ dS_0 \min_{(G,A)\in S_M} \left\{\mathcal{D}_{L}\left((\gamma_0, S_0, \alpha_0), G, A\right)\right\}.
\end{equation}
In practice, the integrals appearing in the above formulas are approximated numerically. The unit square $(\gamma_0, S_0) \in [0,1]^2$ is discretized in a set $\left\{(\gamma_0^i, S_0^{i})\text{ with } i=1, \dots, N_\text{points}\right\}$ which are then summed up with their according weights, \eg for Equation \eqref{eq : app definition fully feasible volume of G, A}:
\begin{equation}
\vol{\mathcal{F}_1^{G,A}(\alpha_0)} \approx \frac{\sum_{i=1}^{N_\text{points}}\mathcal{F}\left((\gamma_0^{i}, S_0^{i}), \alpha_0, G, A\right)}{N_\text{points}}.
\end{equation}

\subsection{Estimate the critical structural perturbation $\Delta_S^*$}\label{app: results structural stability procedure to estimate critical structural perturbation}
As explained in Methods \ref{sec : structural stability methods numerical estimate critical perturbation}, $\Delta_S^*(m, G, A)$ solves the equation:
\begin{equation}
P_E\left(\Delta_S^*(m, G, A), m, G, A\right) = 0.5. \label{eq : structural delta definition}
\end{equation}
The most straight forward way to find $\Delta_S^*$ is to solve Eq.\eqref{eq : structural delta definition} numerically. Fortunately, it turns out that, as expected, $P_E(\Delta_S)$ has a typical sigmoidal shape (Fig.\ref{fig : typical probability observing an extinction}): the transition between $P_E=0$ and $P_E=1$ is sharp compared to the size of the interval $[0,1]$ where $\Delta_S^*$ lies. We wrote a solver for Eq.\eqref{eq : structural delta definition} that exploits this property. It works in the following way:
\begin{enumerate}
\item Through the help of a standard solver from the \code{GSL} library, find a ``high'' $\Delta_H$ for which $P_E(\Delta_H)$ is very close to 1 but smaller, typically $P_E(\Delta_H) \approx 0.99$. Then find a ``low'' $\Delta_L < \Delta_H$, very close to $0$ but larger.
\item Compute $P_E(\Delta_S)$ for $N_{\text{points}}$ points $\Delta_S$ homogeneously spread in the interval $[\Delta_L, \Delta_H]$.
\item Because the $P_E(\Delta_S)$ computed at the previous step typically form a sigmoidal shape, fit these points with a sigmoidal function $S_f(\Delta_S)$. We choose:
\begin{equation}
S_f(\Delta_S) \defined \frac{1}{1+e^{-C_1(\Delta_S-C_2)}}, \label{eq : choice of fitting structural perturbation}
\end{equation}
where the constants $C_1, C_2$ precisely are estimated through the fitting procedure.
\item $\Delta_S^*(m, G, A)$ is obtained by solving analytically $S_f(\Delta_S^*)=0.5$. For the choice of Eq.\eqref{eq : choice of fitting structural perturbation}, this is trivial : $\Delta_S^* = C_2$. Indeed $S_f(C_2)=1/(1+1)=0.5$. We take the error on $C_2$ obtained through the standard fitting routine from the \code{GSL} library as the ``error'' on $\Delta_S^*$.
\end{enumerate}
Finally, it is worth mentioning that $P_E(\Delta_S, m, G, A)$, which again is the probability to observe \important{at least} one extinction when structurally perturbing a parameters set $\mathcal{A}(m,G,A)$ by $\Delta_S$, is estimated numerically through the following procedure:
\begin{enumerate}
\item Create a parameters set $\mathcal{A}(m, G, A)$.
\item Structurally perturb it by an amount $\Delta_S$.
\item Time evolve the parameters set until an equilibrium is reached. Compute $p_E \in \left\{0;1\right\}$, which is $0$ if no extinction has been observed, and $1$ if at least one extinction occurred.
\item Repeat steps 1--3 $N_{\text{sys}}$ times. $P_E(\Delta_S)$ is the average value of $p_E$.
\end{enumerate}
For the figures of Results \ref{sec : results structural stability}, we used $N_\text{points}=125$ and $N_\text{sys}=50$. We observed (although did not have the time to quantify it properly) that increasing $N_\text{points}$ reduces the error on $\Delta_S^*$ faster than increasing $N_\text{sys}$.
\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{{typical_probability_structural_stability_curve}.pdf}
\caption{Typical probability of finding one extinction when structurally perturbing the system with a magnitude $\Delta_S$. The critical structural perturbation is easily estimated with a sigmoidal fit.}\label{fig : typical probability observing an extinction}
\end{figure}
\end{document}
