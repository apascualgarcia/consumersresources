\documentclass[12pt, titlepage]{report}
\usepackage{consumer_resource_final}
\graphicspath{{./figures/}}

\begin{document}
As stated in the introduction, %we are interested in the
our ultimate goal is to study equilibria points of the set of coupled differential equations \eqref{eq: differential eq for resources and species}. In particular we want to know how \important{stable} a given equilibrium is. However there is no consensual definition of stability: what does it mean exactly that a system is stable under a given perturbation? How is a perturbation even defined? %These questions have many different possible answers.
Throughout this thesis different notions of stability will be tackled: the first is \important{dynamical stability}.
The main idea behind dynamical stability is simple. We want to answer the following question:

\begin{centering}
\fbox{\begin{minipage}{\linewidth}
\itshape
Given an equilibrium point $\{ R^*_\mu, S^*_i\}$, does the system go back to a positive-valued equilibrium when the consumers and resources abundances are changed? If yes, how much can they be changed before the system evolves in such a way that it does not reach a positive-valued equilibrium?
\end{minipage}}
\end{centering}
\subsection{Definitions}
\subsubsection{Local dynamical stability}
We first introduce \define{local dynamical stability}. A system is said to be \important{locally dynamically stable} if it goes back to \important{its initial equilibrium point} $\{ R^*_\mu, S^*_i \} $ after $R^*_\mu$ and $S^*_i$ have been perturbed by an infinitesimal amount $\left\{ \Delta R_\mu(t_0), \Delta S_i(t_0) \right\}$ at time $t_0$.

More precisely, consider a system which is at equilibrium at time before $t=t_0$. Right after $t=t_0$, we perturb the equilibria abundances $\left\{R_\mu^*, S_i^*\right\}$ by an infinitesimal amount $\left\{ \Delta R_\mu(t_0), \Delta S_i(t_0) \right\}$.
We want to know how the perturbations away from equilibrium, written $\left\{ \Delta R_\mu(t), \Delta S_i(t) \right\}$, and defined as
\begin{equation}
\Delta R_\mu(t)\defined R_\mu(t)-R_\mu^* \text{ and } \Delta S_i(t) = S_i(t)-S_i^*.
\end{equation}
will evolve qualitatively. Namely, will they go to zero or increase indefinitely as $t$ increases? Perturbation analysis tells us \textbf{insert ref} that the quantity which drives the evolution of $\{ \Delta R_\mu(t), \Delta S_i(t)\}$
is the \define{jacobian matrix of the system at equilibrium} $J^*$, given by :
\begin{equation}
J^* \defined J(t_0),
\end{equation}
where $J(t)$ is the \define{jacobian} of the system \ie the jacobian matrix of its temporal evolution \eqref{eq: differential eq for resources and species} evaluated at time $t$. $J(t)$ has a block matrix structure which is given by:
\begin{equation}
  J(t) \defined
\begin{pmatrix}
  \partiald{\dot{R_\mu}}{R_\nu}& \partiald{\dot{R_\mu}}{S_j} \\
  \partiald{\dot{S_i}}{R_\nu} & \partiald{\dot{S_i}}{S_j}
\end{pmatrix}
=
\begin{pmatrix}
  \left(-m_\mu-\sum_j \gamma_{j\mu}S_j(t)\right)\delta_{\mu\nu} & -\gamma_{j\mu}R_\mu(t)+\alpha_{\mu j} \\
  \sigma_{i\nu}\gamma_{i\nu}S_i(t) &\left(\sum_{\nu} \sigma_{i\nu}\gamma_{i\nu}R_\nu(t)-d_i-\sum_\nu \alpha_{\nu i}\right)\delta_{ij}
\end{pmatrix}, \label{eq: definition of jacobian}
\end{equation}
where $\delta$ is the ubiquitously occurring Kronecker delta symbol defined as:
\begin{equation}
\delta_{ij} =
\begin{cases}
1 \text{ if }i=j, \\
0 \text{ else.}
\end{cases}
\end{equation}
% Using the fact that we are only interested in equilibria where every resource is positive and Eq.\eqref{eq: equilibrium species}, this can be rewritten as:
% \begin{equation}
%  J = \begin{pmatrix}
%    \frac{l_\mu + \sum_j \alpha_{\mu j}S_j^*}{R^*_\mu}\delta_{\mu\nu} & -\gamma_{j\mu}R_\mu+\alpha_{\mu j} \\
%    \sigma_{i\nu}\gamma_{i\nu}S_i &\left(\sum_{\nu} \sigma_{i\nu}\gamma_{i\nu}R_\nu-d_i-\sum_\nu \alpha_{\nu i}\right)\delta_{ij}
%  \end{pmatrix}, \label{eq: definition of jacobian alternative}
% \end{equation}
$J^*$ is then precisely $J$  with $\{R_\mu, S_i\}$ taken at the considered equilibrium point $\{R_\mu^*, S_i^*\}$, which simplifies its structure. Indeed,
since we are interested only in positive valued equilibria (\ie $S^*_i > 0 \ \forall i$), then Eq.\eqref{eq: equilibrium species} is equivalent to:
\begin{equation}
  \sum_\nu \sigma_{i\nu} \gamma_{i\nu}R^*_\nu -d_i - \sum_\nu \alpha_{\nu i} = 0,
\end{equation}
which means that the lower right block of the jacobian in Eq.\eqref{eq: definition of jacobian} will be zero. Hence at equilibrium the jacobian $J^*$ will have the following block form:
\begin{equation}
\boxed{
  J^* = \begin{pmatrix}
  -\Delta & \Gamma \\
  \Beta & 0
\end{pmatrix}
}, \label{eq: jacobian at equilibrium}
\end{equation}
where
\begin{itemize}
  \item $\Delta_{\mu\nu} = \text{diag}(m_\mu+\sum_j \gamma_{j\mu} S^*_j) = \text{diag}\left(\frac{l_\mu + \sum_j \alpha_{\mu j}S_j^*}{R^*_\mu}\right)$ is a positive $N_R \times N_R$ diagonal matrix,
  \item $\Gamma_{\mu j} = -\gamma_{j\mu}R^*_\mu + \alpha_{\mu j}$ is a $N_R \times N_S$ matrix which does not have entries with a definite sign.
  \item $\Beta_{i\nu} = \sigma_{i\nu} \gamma_{i\nu} S^*_i$ is a $N_S \times N_R$ matrix with positive entries.
\end{itemize}
For reasons explained later in the manuscript, we say that a given equilibrium is \define{locally dynamically stable} if the largest real part of the eigenvalues of $J^*$ is negative.


\subsubsection{The locally dynamically stable volume $\mathcal{D}^{G,A}_{L,x}$} \label{sec: dynamical stability methods locally dynamically stable region}
Similarly to what was conducted in Methods \ref{sec : methods feasibility}, one can define the \define{parameters set local dynamical stability function} $\mathfrak{D}_L: \mathcal{P} \rightarrow \{
0,1\}$, which tells you whether a given set of parameters $p \in \mathcal{P}$ is locally dynamically stable or not:
\begin{equation}
\mathfrak{D}_L(p)\defined\begin{cases}
1 \text{ if } p \text{ is locally dynamically stable} \\
0 \text{ else.}
\end{cases}
\end{equation}
Of course, $p$ has to be feasible in order to be locally dynamically stable:
\begin{equation}
\mathfrak{D}_L(p)=1 \implies \mathfrak{F}(p)=1. \label{eq: locally dynamically stable implies feasible}
\end{equation}
We also define the \define{metaparameters set local dynamical stability function} $\mathcal{D}_L: \mathcal{M} \times \mathcal{B}_{N_S \times N_R} \times \mathcal{B}_{N_R \times N_S} \rightarrow [0,1]$ which tells you, given a set of metaparamters $m \in \mathcal{M}$ and a consumption-syntrophy network $B=(G,A)$ the chance that the procedure $\mathcal{A}(m,B)$ gives a locally dynamically stable set of parameters:
\begin{equation}
\boxed{\mathcal{D}_L(m, B)\defined\text{Probability}\left\{\mathfrak{D}_L(\mathcal{A}(m, B))=1\right\}}.
\end{equation}
We also define the $x$ locally dynamically stable (lds) region $\mathcal{D}_{L,x}^{G,A}$ by the region of the metaparameters space that gives rise to a percentage of at least $x$ dynamically stable systems:
\begin{equation}
\mathcal{D}_{L,x}^{G,A} \defined \left\{m \in \mathcal{M}: \mathcal{D}_L(m, (G,A)) \geq x \right\}
\end{equation}
Clearly, $\mathcal{D}_{L,0}^{G,A}=\mathcal{M}$, $\text{Vol}\left(\mathcal{D}_{L,x}^{G,A}\right) \leq \text{Vol}\left(\mathcal{D}_{L,y}^{G,A}\right)$ $\forall x \geq y$, and more importantly, Eq.\eqref{eq: locally dynamically stable implies feasible} is equivalent to $\mathcal{D}_{L,x}^{G,A} \subset \mathcal{V}_x^{G,A}$. We can also define for a set of $N$ couples of matrices $S=\left\{(G_1, A_1) \dots, (G_N, A_N)\right\}$ their common $x$ lds-region $\mathcal{D}_{L,x}^S$:
\begin{equation}
\mathcal{D}_{L,x}^S \defined \intersection{(G,A) \in S} \mathcal{D}_{L,x}^{G,A}.
\end{equation}
For such a set $S$ we define also its critical local dynamical stability $d_L^*(S)$ which is the largest local dynamical stability we can achieve while still having a non-zero common volume:
\begin{equation}
d_L^*(S) = \max_{x \in [0,1]}\left\{x: \text{Vol}(\mathcal{D}_{L,x}) > 0\right\}.
\end{equation}
Finally the critical common local dynamical stability volume $\mathcal{D}^S_{L}$ is the common lds-region at the critical local dynamical stability:
\begin{equation}
\mathcal{D}^*_L \defined \mathcal{D}_{L, d_L^*(S)}^S.
\end{equation}
The hope is that we can work most of the time with systems that have $d_L^*(S)=1$.

\subsubsection{Global dynamical stability}
If we establish that a system is locally dynamically stable, we know that it will come back to the same equilibrium after an infinitesimal perturbation of the resources and consumers abundances. The next natural question is:


\begin{centering}
\fbox{\begin{minipage}{\linewidth}
\itshape \important{How much} can these equilibria points be perturbed before the system goes to a point where either at least a species has gone extinct or reaches another positive valued equilibrium $\{ \tilde{R}^*_\mu, \tilde{S}^*_i\}$ or simply does not reach a new dynamical equilibrium?
\end{minipage}}
\end{centering}


\noindent One way of studying this \cite{pascual-garcia_mutualism_2017} is to simply take an equilibrium point $\{ R^*_\mu, S^*_i\}$ and perturb the abundance of the species and resources at that point by a fixed number $\Delta_D \in \left[0, 1\right]$ which allows us to quantify the perturbation:
\begin{empheq}[left=\empheqlbrace]{align}
  R^*_\mu \rightarrow R_\mu(t_0) \equiv  R^*_\mu \left(1+\Delta_D \nu_\mu\right), \\
  S^*_i \rightarrow S_\mu(t_0) \equiv S^*_i \left(1+\Delta_D \nu_i \right),
\end{empheq}
where the $\nu_{\mu, i}$ are random numbers drawn from a uniform distribution between -1 and +1 and $t_0$ is the time where the previously at equilibrium system is perturbed.
The system with the initial values $\{R(t_0), S(t_0)\}$ can then be time evolved from $t=t_0$ until it reaches an equilibrium $\{\tilde{R}^{*}, \tilde{S}^{*}\}$ which may be different from the equilibrium $\{R^*, S^*\}$ initially considered.
This procedure is essentially a generalized version of local dynamical stability, since we allow the perturbation $\Delta_D$ to be non-infinitesimal. The question we will ask is precisely how big $\Delta_D$ can get.


A certain number of quantities, that all depend on the perturbation $\Delta_D$, can then be measured to quantify the dynamical stability of the system:
\begin{itemize}
  \item The resilience $t_R$: the time scale over which the system reaches its new equilibrium.
  \item The number of extinctions $E$: the number of species or resources which died during the time it took the system to reach its new equilibrium.
  \item The angle $\alpha$ between two equilibria: this quantifies how close the old and new equilibria are. $\alpha$ is defined through its standard scalar product formula:
  \begin{equation}
  \cos(\alpha) \equiv \frac{\sum_\mu R^*_\mu \tilde{R}^*_\mu + \sum_j S^*_j\tilde{S}^*_j}{\sqrt{\sum_\mu \left(R^*_\mu\right)^2 + \sum_i \left(S^*_i\right)^2}\sqrt{\sum_\mu \left(\tilde{R}^*_\mu\right)^2 + \sum_i \left(\tilde{S}^*_i\right)^2}}.
  \end{equation}
\end{itemize}
These quantities have either been already introduced in previous papers or are natural extensions of standard quantities \cite{ives_stability_2007,pascual-garcia_mutualism_2017}. They allow us to quantify the robustness of a given equilibrium.

\subsection{The quest for a full solution}
The question of global dynamical stability is mathematically tedious, so we focus on local dynamical stability.
We aim to find the spectrum of the jacobian at equilibrium, which will tell us whether the system is locally dynamically stable or not.

\subsubsection{How to determine local dynamical stability}
We stated above that the sign of the largest real part of all the eigenvalues of $J^*$ determines the local dynamical stability.
More precisely, we are interested in the real part of $\lambda_1$, which is defined by the following property:
\begin{equation}
\boxed{\forall \lambda \in \sigma(J^*), \ \real{\lambda} \leq \real{\lambda_1}},
\end{equation}
where $\sigma(J^*)$ is the set of eigenvalues of $J^*$, called the \define{spectrum} of $J^*$. Perturbation analysis tells us that the sign of the real part of $\lambda_1$ governs the local stability of the system at equilibrium \textbf{add source}. There are three cases:
\begin{itemize}
\item $\real{\lambda_1} < 0$: any perturbation on the abundances is exponentially supressed. The system is stable.
\item $\real{\lambda_1} > 0$: any perturbation on the abundances is exponentially amplified. The system is unstable.
\item $\real{\lambda_1}=0$: a second order perturbation analysis is required to assess the system's local dynamical stability. We call such systems \textit{marginally stable} \cite{biroli_marginally_2018}.
\end{itemize}

\subsubsection{The master equation for local dynamical stability}
In order to get $\real{\lambda_1}$, we have to get the full spectrum of $J^*$, as a straight forward application of easier standard techniques like the Perron-Frobenius theorem \cite{perron_zur_nodate} does not work. The eigenvalues of $J^*$ are obtained through the eigenvalue problem:
\begin{equation}
\det\left(J^* - \lambda \right) = 0.
\end{equation}
More explicitly, using Eq.\eqref{eq: jacobian at equilibrium}, we state the \important{master equation for local dynamical stability}:
\begin{equation}
\boxed{
\det
\begin{pmatrix}
 -\Delta - \lambda  & \Gamma \\
 \Beta & 0-\lambda
\end{pmatrix} = 0
}\label{eq: master equation eigenvalues jacobian}
\end{equation}
% Before proceeding any further, we eliminate systems where local dynamical stability cannot be decided with the first order perturbation analysis that we are conducting, \ie marginally stable equilibria.
That equation is not trivially solved. We then seek regimes where it could be made simpler.
% \subsubsection{Marginally stable equilbria}
% We want to avoid the case $\real{\lambda_1}=0$, because it does not let us assess the system's stability without a further complicated mathematical analysis. To make things easier, we will in analytical computations\footnote{For numerical computations we will get rid of marginally stable systems individually, meaning we may have some systems where $0 \in \sigma(J^*)$.} get rid of all systems where 0 is part of the spectrum. Even though this is a harsh condition, we know that for such systems, local dynamical stability can be decided by the computations we conduct. $\lambda=0$ is part of the spectrum if and only if it solves the master equation \eqref{eq: master equation eigenvalues jacobian}:
% \begin{equation}
% \det
% \begin{pmatrix}
%   -\Delta   & \Gamma \\
%   \Beta & 0
% \end{pmatrix} = 0 \label{eq: determinant marginally stable equilibria}
% \end{equation}
% Using the fact that $\Delta$ is invertible, we can make use of the equality\footnote{This uses a formula which is trivially analogous to one found in \cite{powell_calculating_2011}.}:
% \begin{equation}
% \det\begin{pmatrix}
%   -\Delta   & \Gamma \\
%   \Beta & 0
% \end{pmatrix} = \det(-\Delta)\det(\Beta\Delta^{-1}\Gamma).
% \end{equation}
% Eq.\eqref{eq: determinant marginally stable equilibria} then becomes:
% \begin{equation}
% \det(\Beta\Delta^{-1}\Gamma)=0
% \end{equation}
% which means that $\Beta \Delta^{-1}\Gamma$ is not full rank. We can hence state the following
\subsubsection{Simplifying the master equation}\label{section: non marginal equilibria}
% For now we will concentrate on equilibria that are clearly either stable or unstable\footnote{The case of marginally stable systems, where the maximum eigenvalue is zero, will be covered later.}, \ie:
% \begin{equation}
% \lambda_1 \neq 0.
% \end{equation}
Equation \eqref{eq: master equation eigenvalues jacobian} may be simplified if
\begin{equation}
\lambda \neq 0.
\end{equation}
Indeed\footnote{Appendix \ref{sec : zero part of spectrum} elaborates on when that condition is fulfilled.}, a non-zero $\lambda$ implies
\begin{equation}
\det\left(\lambda \identity{N_S}\right)\neq 0,
\end{equation}
where $\identity{N_S}$ stands for the $N_S \times N_S$ identity matrix. One can use this condition to simplify Eq.\eqref{eq: master equation eigenvalues jacobian} using the properties of block matrices \cite{powell_calculating_2011}:
\begin{equation}
\det
\begin{pmatrix}
  -\Delta - \lambda \identity{N_R}  & \Gamma \\
  \Beta & 0-\lambda \identity{N_S}
\end{pmatrix} =
\det\left(-\lambda \identity{N_S}\right)\det\left(-\Delta- \lambda\identity{N_R}+\frac{1}{\lambda}\Gamma \Beta\right).
\end{equation}
Hence Eq.\eqref{eq: master equation eigenvalues jacobian} becomes:
\begin{equation}
\boxed{
\det\left(\lambda^2 \identity{N_R}+\Delta \lambda-\Gamma \Beta\right)=0. \label{eq: master equation non marginal}
}
\end{equation}
The complexity here is already reduced because we go from the determinant of a $N_R+N_S$ square matrix to a $N_R$ square matrix. We see from the previous expression that the dynamics is essentially dictated by the $\Gamma \Beta$ $N_R$-dimensional square matrix, which is given by:
\begin{equation}
\left(\Gamma \Beta\right)_{\mu \nu} = \sum_i \Gamma_{\mu i} \Beta_{i \nu} = \sum_i \left(\alpha_{\mu i}-\gamma_{i \mu} R^*_\mu \right)\sigma_{i\nu}\gamma_{i\nu}S^*_i.
\end{equation}
There are many strategies here to find regimes of stability. One is the so-called ``Reductio ad absurdum'', which is explored later in Methods \ref{subsubsec: reductio ad absurdum}.

\subsection{Bounds on the eigenvalues}
Before studying Equation \eqref{eq: master equation non marginal}, we would like to know more about the spectrum of $J^*$. The most critical question is knowing \important{where} we expect the eigenvalues of $J^*$ to lie on the complex plane.
\subsubsection{Gerschgorin circle theorem}
Gerschgorin circle theorem \cite{gerschgorin_uber_1931}
%allows us to get a better idea of the location of the eigenvalues in the complex plane. It
states that every eigenvalue of a $N\times N$ square matrix $A$
is located in one of the $N$ discs $D_i$ defined by:
\begin{equation}
D_i\defined \left\{z \in \mathbb{C}: \abs{z-A_{ii}} \leq \sum_{j\neq i} \abs{A_{ij}} \right\}. \label{eq: definition discs Gerschgorin}
\end{equation}
In a more mathematical language:
\begin{equation}
\sigma(A) \subset \union_{i=1}^N D_i. \label{eq: circle theorem}
\end{equation}
Intuitively, the circle theorem tells us that the eigenvalues of a matrix deviate from the diagonal elements by a value bounded by the sum of the off-diagonal elements.
It is then easy to see that if all the discs $D_i$ are located to the left of the imaginary axis (\ie the discs contain only numbers with a negative real part), then the eigenvalues of $A$ are all negative. This corresponds to the following lemma:
\begin{lemma}\label{lemma: lemma Gerschgorin circle}
If a $N-$dimensional square matrix $A$ verifies the equations:
\begin{equation}
\real{A_{ii}} + \sum_{j\neq i} \abs{A_{ij}} < 0, \forall \ i=1, \dots, N, \label{eq: alternative version circle theorem}
\end{equation}
then $\real{\lambda} < 0 \ \forall \lambda \in \sigma(A)$.
\end{lemma}
\begin{proof}
Let $\lambda \in \sigma(A)$. By the circle theorem, there exists $k \in \left\{1,\dots, N\right\}$ such that :
\begin{equation}
\abs{\lambda-A_{kk}} \leq \sum_{j\neq k} \abs{A_{kj}}.
\end{equation}
We now use the complex identity:
\begin{equation}
\abs{\lambda-A_{kk}} \geq \real{\lambda-A_{kk}} = \real{\lambda}-\real{A_{kk}}.
\end{equation}
Equation \eqref{eq: alternative version circle theorem} implies:
\begin{equation}
\sum_{j\neq k}\abs{A_{kj}} < - \real{A_{kk}}.
\end{equation}
Combining the two previous inequalities yields:
\begin{equation}
\real{\lambda}-\real{A_{kk}} \leq \abs{\lambda-A_{kk}} \leq \sum_{j\neq k} \abs{A_{kj}} < -\real{A_{kk}}.
\end{equation}
Comparing the RHS and LHS of this inequality yields:
\begin{equation}
\real{\lambda} < 0.
\end{equation}
\end{proof}
\noindent The Gerschgorin circle theorem allows us to get a precious bound on the modulus of each eigenvalue and hence on the one that decides the dynamics of the system $\lambda_1$. Indeed we know that all eigenvalues of $J^*$ will be located in one of the $N_R + N_S$ discs of $J^*$. These are the ``resources'' discs:
\begin{equation}
D^R_\mu  \defined \left\{ z \in \mathbb{C}: \abs{z+\Delta_\mu} \leq \sum_j \abs{\Gamma_{\mu j}} \right\}  \ \forall \mu = 1, \dots, N_R,
\end{equation}
and the ``consumers'' discs:
\begin{equation}
D^C_i \defined \left\{ z \in \mathbb{C}: \abs{z} \leq \sum_\nu \abs{B_{i\nu}}\right\} \ \forall i=1, \dots, N_S.
\end{equation}
According to the circle theorem Eq.\eqref{eq: circle theorem}, all eigenvalues will be in the union of these circles, \ie there exists $\forall \lambda \in \sigma\left(J^*\right)$ at least one $\mu^*$ or  one $i^*$ such that:
\begin{equation}
\abs{\lambda} \leq \sum_\nu \abs{B_{i^*\nu}} \label{eq: bound lambda consumers}
\end{equation}
or
\begin{equation}
\abs{\lambda+\Delta_{\mu^*}} \leq \sum_j \abs{\Gamma_{\mu^* j}} \label{eq: bound lambda 1 resources}
\end{equation}
The triangle inequality implies:
\begin{equation}
\abs{\lambda} \leq \abs{\lambda+\Delta_{\mu^*}}+\abs{-\Delta_{\mu^*}} \leq \sum_j \abs{\Gamma_{\mu^* j}} + \abs{-\Delta_{\mu^*}} = \sum_j \abs{\Gamma_{\mu^* j}} + \Delta_{\mu^*}.\label{eq: bound lambda resources better}
\end{equation}
The only way both Eq.\eqref{eq: bound lambda consumers} and \eqref{eq: bound lambda resources better} are satisfied for all eigenvalues, and especially the one with the highest real part $\lambda_1$ is if they are bound by the maximum of both RHS of these equations. More precisely:
\begin{equation}
\boxed{
\abs{\lambda} \leq R_C \ \forall \lambda \in \sigma(J^*), \label{eq: overall bound on lambda}
}
\end{equation}
where we defined the critical radius $R_C$ as:
\begin{equation}
R_C \defined \max\left\{\max_i\left\{\sum_\nu \abs{B_{i\nu}}\right\}, \max_\mu\left\{\sum_j \abs{\Gamma_{\mu j}}+\Delta_\mu\right\}\right\}.
\end{equation}
This gives us an estimation of how big the eigenvalues can get: we know that all the eigenvalues \important{have} an absolute value smaller than or equal to the critical radius $R_C$.
The next step is to estimate $R_C$ in terms of metaparameters, so that we can get a qualitative insight on how the eigenvalues change when the metaparameters are changed.

\noindent Using techniques very similar to previous computations, we estimate:
\begin{equation}
\sum_j \abs{\Gamma_{\mu j}}+\Delta_\mu = \sum_j \abs{\alpha_{\mu j}-\gamma_{j\mu}R^*_\mu}+\frac{l_\mu+\sum_j\alpha_{\mu j} S^*_j}{R^*_\mu} \approx \deg(\Gamma, \mu)\abs{\alpha_0 - \gamma_0 R_0}+\frac{l_0+\deg(A,\mu)\alpha_0 S_0}{R_0}.
\end{equation}
It is difficult to simplify $\deg(\Gamma, \mu) \approx \deg(A-G^T, \mu)$. If we assume that $A$ and $G$ have a low connectance then $\deg(A, \mu), \deg(G^T, \mu) \ll N_S$ and we may use the very loose approximation
\begin{equation}
\deg(A-G^T, \mu) \approx \deg(A, \mu)+\deg(G, \mu).
\end{equation}
In that regime we then have:
\begin{equation}
\max_\mu\left\{\sum_j \abs{\Gamma_{\mu j}}+\Delta_\mu\right\} \approx \max_\mu\left\{\left(\deg(A, \mu)+\deg(G, \mu)\right)\abs{\alpha_0-\gamma_0 R_0}+\frac{l_0+\deg(A,\mu)\alpha_0 S_0}{R_0}\right\}.
\end{equation}
Similarly we find
\begin{equation}
\sum_\nu \abs{B_{i\nu}} \approx \deg(G, i) \sigma_0 \gamma_0 S_0,
\end{equation}
such that $R_C$ can be estimated roughly as:
\begin{multline}
R_C \approx \max\left\{ \max_i\left(\deg(G,i)\right) \sigma_0 \gamma_0 S_0 \right., \\
 \left.\max_\mu\left\{\left(\deg(A, \mu)+\deg(G, \mu)\right)\abs{\alpha_0-\gamma_0 R_0}+\frac{l_0+\deg(A,\mu)\alpha_0 S_0}{R_0}\right\}\right\} \label{eq: estimate R_C metaparameters}
\end{multline}
% The main factor that will determine $R_C$ (and hence the largest magnitude of any eigenvalue) is the structure of the food consumption matrix.


\subsection{Low intra resources interaction (LRI) regime }
Now that we have a bound on how big the eigenvalues can be, we need strategies to find regimes where we \important{know} $\real{\lambda_1} < 0 $, \ie local dynamical stability is guaranteed. We inspire ourselves from the general idea of the mathematical proofs of \cite{butler_stability_2018}.
\subsubsection{Reductio ad absurdum} \label{subsubsec: reductio ad absurdum}
Using standard properties of determinants, we rewrite Eq.\eqref{eq: master equation non marginal} as\footnote{We can do this because since $m_\mu > 0$, we know $\Delta$ will always be invertible.}:
\begin{equation}
\det\left(-\Delta^{-1}\right)\det\left(-\Delta^{-1}\lambda^2-\lambda+\Delta^{-1}\Gamma\Beta\right)=0\iff \boxed{\det\left(S(\lambda)-\lambda\right)=0} \label{eq: eigenvalue problem S}
\end{equation}
with
\begin{equation}
\boxed{S(\lambda)=\Delta^{-1}\Gamma\Beta-\Delta^{-1}\lambda^2}, \label{eq: equation stability S}
\end{equation}
or, component-wise:
\begin{equation}
S_{\mu \nu} = \frac{1}{\Delta_\mu}\left[\left(\sum_i \Gamma_{\mu i}\Beta_{i \nu}\right) - \lambda^2 \delta_{\mu\nu}\right] \label{eq: definition S component wise}
\end{equation}
The strategy is to assume we are in an unstable regime,\ie there exists at least one $\lambda \in \sigma(J^*)$ with $\real{\lambda}\geq 0$ that satisfies Eq.\eqref{eq: master equation non marginal} and such that $\real{\lambda} > 0$. By Eq.\eqref{eq: eigenvalue problem S}, $\lambda$ is also an eigenvalue of $S(\lambda)$. If we find conditions under which the real part of the spectrum of $S(\lambda)$ is entirely negative, we will know that $\real{\lambda} \leq 0$. As this is a contradiction to the hypothesis that the regime is unstable, we must conclude that the regime is stable\footnote{Indeed, Eq.\eqref{eq: master equation non marginal} assumes already that either $\real{\lambda_1} >0$ or $\real{\lambda_1} < 0$.}.

Hence, the general idea is to find regimes where we know that the spectrum of $S$, written as $\sigma(S)$, will be entirely negative for a positive $\lambda$. We found such a regime, which we can enunciate as two theorems.
\subsubsection{Strong LRI regime}
\begin{theorem}\label{theorem: strong LRI regime}
Let $p$ be a parameter set with a jacobian at equilibrium $J^*$. If $0$ is not an eigenvalue of $J^*$ and the equations
\begin{equation}
\left(\Gamma \Beta\right)_{\mu\mu} < - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}} - R_C^2 \ \forall \mu,
\end{equation}
are verified, then $p$ is dynamically stable.
\end{theorem}
\begin{proof}
We assume
\begin{equation}
\left(\Gamma \Beta\right)_{\mu\mu} < - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}} -R_C^2 \ \forall \mu.
\end{equation}
This implies:
\begin{equation}
\left(\Gamma \Beta\right)_{\mu\mu} + R_C^2< - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}} \ \forall \mu.
\end{equation}
Using Eq.\eqref{eq: overall bound on lambda} and $\imag{\lambda}^2 \leq \abs{\lambda}^2$, we get:
\begin{equation}
  \left(\Gamma\Beta \right)_{\mu\mu} + \imag{\lambda}^2 < - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}} \ \forall \mu. \label{eq: low species bound 1 strong}
\end{equation}
It is not difficult to prove that for any complex number:
\begin{equation}
\imag{c}^2 \geq - \real{c^2} \ \forall c \in \mathbb{C}.
\end{equation}
Using this result and dividing Eq.\eqref{eq: low species bound 1 strong} by\footnote{That step is valid because $\Delta_\mu > 0$, $\forall \mu$.} $\Delta_\mu$, we get:
\begin{equation}
\frac{1}{\Delta_\mu}\left[\left(\sum_i \Gamma_{\mu i} \Beta_{i \mu} \right)-\real{\lambda^2}\right] < - \sum_{\nu \neq \mu } \abs{\frac{\sum_i \Gamma_{\mu i}\Beta_{i\nu}}{\Delta_\mu}} \ \forall \mu. \label{eq : component wise LRI strong}
\end{equation}
Looking at Eq.\eqref{eq: definition S component wise}, we see that Eq.\eqref{eq : component wise LRI strong} is equivalent to:
\begin{equation}
\real{S_{\mu \mu}} + \sum_{\nu \neq \mu} \abs{S_{\mu \nu}} < 0\ \forall \mu.
\end{equation}
Lemma \ref{lemma: lemma Gerschgorin circle} implies that all the eigenvalues of $S(\lambda)$ have a negative real part.
As explained before that means that if $\real{\lambda_1} \geq 0 $ in Eq.\eqref{eq: equation stability S} (unstable or marginally stable regime), then $\real{\lambda_1} < 0$, which leads to a contradiction. This then implies that the equilibrium is dynamically stable.
\end{proof}
\subsubsection{Weak LRI regime}
A weaker version of Theorem \ref{theorem: strong LRI regime} can be stated. Its proof is in Appendix \ref{sec : weak LRI regime}. 
\begin{theorem}\label{theorem: weak LRI regime}
Let $p$ be a parameter set with a jacobian at equilibrium $J^*$. If $0$ is not an eigenvalue of $J^*$ and the equations
\begin{equation}
\left(\Gamma \Beta\right)_{\mu\mu} < - \sum_{\nu \neq \mu } \abs{\left(\Gamma \Beta\right)_{\mu \nu}} \ \forall \mu,
\end{equation}
are verified, then the real eigenvalues of $J^*$ are negative.
\end{theorem}




\subsubsection{Feasibility of the low intra resources interaction regime}
So we found that if a system has parameters that respect Eq.\eqref{eq: LRI regime} then it is dynamically stable. A naturally arising question is then to ask in what measure this is compatible with the feasability equations Eqs.\eqref{eq: feasability energy conservation}, \eqref{eq: feasability positivity d} and \eqref{eq: feasability positivity m}.

\noindent Finding an approximation of the resource interaction matrix $(\Gamma\Beta)_{\mu \nu}$ using the metaparameters allows to find a necessary condition on the metaparameters. Indeed, using the metaparameters approximations Eq.\eqref{eq: metaparameters approximations}, we get:
\begin{equation}\label{eq: metaparam approximation GammaBeta}
\left(\Gamma \Beta\right)_{\mu \nu}\approx \sigma_0 \gamma_0 S_0 \left(\alpha_0 \sum_i A_{\mu i}G_{i\nu}-\gamma_0 R_0 \sum_i G_{i\mu}G_{i\nu}\right)\defined \sigma_0 \gamma_0 S_0 \left(\alpha_0 O_{\mu \nu}-\gamma_0R_0 C_{\mu\nu}\right)
\end{equation}
where we defined the syntrophy overlap matrix $O_{\mu\nu}$ and the consumption overlap matrix $C_{\mu\nu}$ as:
\begin{equation}
O_{\mu\nu} \defined \left(AG\right)_{\mu \nu} \text{ and } C_{\mu\nu} \defined \left(G^TG\right)_{\mu\nu}.
\end{equation}
The fight syntrophy vs. consumption between these two binary matrices essentially builds the dynamics of our model and an intuition about their meaning can be very helpful.

The syntrophy overlap matrix $O_{\mu\nu}$ is defined as:
\begin{equation}
O_{\mu\nu} \defined \sum_k A_{\mu k} G_{k\nu}.
\end{equation}
Although $A$ and $G$ are binary, $O$ does not have to and usually won't be. A given consumer $k$ contributes to $O_{\mu\nu}$ if and only if both $A_{\mu k}$ and $G_{k\nu}$ are non zero, that is if consumer $k$ releases resource $\mu$ \textit{and} consumes resource $\nu$. Hence $O_{\mu\nu}$ essentially tells how many species effectively link resource $\mu$ to resource $\nu$ through the indirect interaction of the species consumption.

\noindent Similarly, the consumption overlap matrix is defined as:
\begin{equation}
C_{\mu\nu}=\sum_k G_{k \mu}G_{k \nu}.
\end{equation}
Like $O$, $C$ usually will not be binary. The intuition behind $C_{\mu\nu}$ is straight forward: it counts how many species eat both resource $\mu$ and $\nu$. Note that $C_{\mu\nu}=C_{\nu\mu}$ (interesting: hard part comes from the antisymmetric part of S).

We then find a lowerbound for the RHS of Eq.\eqref{eq: LRI regime}:
\begin{equation}
-\sum_{\nu\neq\mu}\abs{\Gamma\Beta}_{\mu\nu} \geq - \sum_{\nu \neq \mu} \max_{\nu \neq \mu} \abs{\Gamma\Beta}_{\mu\nu} \geq - \deg\left(\mu, O-C \right) \max_{\nu \neq \mu} \abs{\Gamma\Beta}_{\mu\nu}.
\end{equation}
Combining this with the approximation of $\Gamma\Beta$ above we get an approximative LRI regime condition on the metaparameters:
\begin{equation}\boxed{
\alpha_0 O_{\mu \mu}-\gamma_0R_0 C_{\mu\mu} \lessapprox - \deg(\mu, O-C) \max_{\nu\neq\mu}\abs{\alpha_0 O_{\mu \nu}-\gamma_0 R_0 C_{\mu\nu}}-\frac{R_C^2}{\sigma_0\gamma_0 S_0} \ \forall \mu.
}\label{eq: LRI metaparams sufficient}
\end{equation}
Since $R_C$ essentially scales with the largest degree of $G$ (see Eq.\ref{eq: estimate R_C metaparameters}) we only expect systems with a low connectance food consumption adjacency matrix to be able to achieve an LRI state.

This allows to give a necessary condition on the magnitude of $\alpha_0$. Indeed, since the RHS of the previous equation is negative, we need:
\begin{equation}
\alpha_0 O_{\mu \mu}-\gamma_0 R_0 C_{\mu\mu} < 0 \ \forall \mu \implies {\alpha_0 \max_\mu \left\{ \frac{O_{\mu\mu}}{C_{\mu\mu}}  \right\} < \gamma_0 R_0 .} \label{eq: LRI necessary maximum alpha0}
\end{equation}
It is clear that systems with the maximal ratio of $O_{\mu\mu}$ and $C_{\mu\mu}$ is small will be more easily into a LRI regime. The most favoured systems will be those where $S_{\mu\mu}=0 \ \forall \mu$, \ie systems where no species consumes what it itself produces. In that way we may say that coprophagy tends to destabilize microbial communities.
Combining Eq.\eqref{eq: feasability positivity m} and \eqref{eq: LRI necessary maximum alpha0} gives us a necessary condition on $\alpha_0$ for feasible systems (need more details?):
\begin{equation}
\boxed{
\alpha_0 \left[\max_\mu \left\{O_{\mu\mu}\right\}-\min_\mu \left\{ \frac{k^\alpha_\mu}{k^\gamma_\mu} \right\} \right] \leq \frac{l_0}{\min_\mu\left(k_\mu^\gamma\right)S_0}}.
\end{equation}
Although that equation gives us a necessary condition, it is not sufficient. Eq.\eqref{eq: LRI metaparams sufficient}, on the other hand, is and provides an intuitive way of finding a syntrophy adjacency matrix $A_{\mu i}$ that would put a system with a given consumption adjacency $G_{\mu i}$ in an LRI regime. Section \ref{section: numerical analysis LRI MC solver} explains in details how this can be achieved numerically.

\subsubsection{Monte Carlo algorithm for the optimal syntrophy matrix} \label{section: methods LRI MC solver}
We want to find a general algorithm which, for a given food consumption adjacency matrix $G$ gives back an optimal syntrophy adjacency matrix $A$. Strategically, we would like an $A$ such that Eq.\eqref{eq: LRI metaparams sufficient} is as close to being satisfied as possible. If it were satisfied, it would put the system in an LRI regime, which we have proven is dynamically stable.

One way of trying to satisfy Eq.\eqref{eq: LRI metaparams sufficient} is to increase the magnitude of its LHS and minimize the magnitude of the RHS. The LHS is minimized if $(AG)_{\mu\mu}$ is set to its lowest possible value for every $\mu$, that is zero. On the other hand, the RHS is minimized if $\alpha_0(AG)_{\mu\nu}\approx\gamma_0R_0 (G^TG)_{\mu\nu} \ \forall \nu\neq\mu$.

Intuitively, we then search for systems where $AG$ is zero on the diagonal, \ie where no coprophagy is observed, and $AG \approx \frac{\gamma_0R_0}{\alpha_0}G^TG$ outside the diagonal. It can be formalized by writing a proper Metropolis-Hastings Markov Chain Monte Carlo (MCMC) method. We designed the following algorithmic procedure to build a syntrophy adjacency matrix $A$:
\begin{enumerate}
\item Create a random $A$. Its connectance is chosen as the one of the consumption matrix $G$.
\item Do the following for a given number of steps:
\begin{itemize}
\item Choose a random row or, every other iteration, a column.
\item In that row/column, try to swap a zero and a one while preserving the ``releasers'': if a species releases some resource, it has to keep releasing something (the resource can change though). The ``releasees'' are preserved as well: if a resource is being released by some species, it has to keep being released (but it does not have to be by the same species). \textbf{why do we impose those conditions?}
\item The swap is accepted, \ie $A$ is modified, if the energy difference $\Delta E$ is negative or if a random number drawn uniformly between zero and one is smaller than $e^{-\Delta E/T}$ where $T$ is the current temperature. More on $\Delta E$ and $T$ below.
\end{itemize}
\item Return $A$.
\end{enumerate}
A couple comments on this algorithm can be made:
\begin{itemize}
\item The algorithm preserves the connectance of $A$ but not its nestedness. The question of what value to choose is open, but we choose $\kappa(A)=\kappa(G)$ as a first approach, \ie syntrophy and consumption networks have the same connectance.
\item The temperature $T$ changes dynamically during the simulation. It is obtained in a way close to the spirit of simulated annealing techniques \cite{gendreau_simulated_2019}: the temperature $T$ is multiplied by a factor $\lambda=0.99$ at a fixed frequency (for instance every 1000 steps). We add the requirement that if new moves are rejected during too many consecutive steps, we multiply the temperature by $1/\lambda$.
\item The energy difference $\Delta E$ between the new proposed $A'$ and the old $A$ is computed by assigning an energy $E$ to both $A'$ and $A$ and subtracting them:
\begin{equation}
\Delta E \defined E(A', G)-E(A, G).
\end{equation}
The choice of the energy function $E$ is crucial. In essence, this MCMC algorithm will find the specific $A$ which minimizes $E(A)$. Since we want to work with systems in the LRI regime, we use the simplest and most natural function that is compatible with the intuitively expected characteristics of $A$ explained above (\ie $AG$ is zero on the diagonal and equal to $\frac{\gamma_0 R_0}{\alpha_0} G^TG$ outside of it):
\begin{equation}
E(A,G) \defined \sum_\mu \left( \abs{\alpha_0(AG)_{\mu\mu}} + \sum_{\nu\neq\mu} \abs{(\alpha_0 AG-\gamma_0 R_0 G^TG)_{\mu\nu} }\right). \label{eq: dynamical stability methods LRI MC solver energy definition}
\end{equation}
The energy function and hence the optimal syntrophy adjacency matrix $A$ depend on the ratio $\frac{\alpha_0}{\gamma_0 R_0}$. This prompts then the question of which $\alpha_0$ can be deemed sensible. As a first step, we will take the value of Eq.\eqref{eq: largest feasible alpha0}: $\alpha_0 = \min(1-\sigma_0, \sigma_0) \gamma_0 R_0 N_R$. This means that the outcome of the algorithm is an optimized $A$ \textbf{for the largest feasible syntrophy}. Since the expression we have for the largest feasible syntrophy is independent of the $G$ matrix, this choice of $\alpha_0$ provides us a sensible way of comparing different consumption networks.
\end{itemize}



% \paragraph{LRI state for an identity food consumption matrix}
% We would like to know which systems can be in an LRI regime. Consider here the case of $N_R=N_S$ and every species consumes exactly one resource, to each its own. This means the food consumption adjacency matrix is precisely the identity matrix:
% \begin{equation}
% G_{i\mu} = \delta_{i\mu}.
% \end{equation}
% We can then compute explicitly the $\Gamma\Beta$ matrix:
% \begin{equation}
% \left(\Gamma \Beta\right)_{\mu \nu} = \sum_i \left(\alpha_{\mu i}- \gamma_{i\mu}R_\mu^*\right) \sigma_{i\nu}\gamma_{i\nu}S_i^*=\delta_{\mu\nu}\left(\alpha_{\mu\nu}-\gamma_{\nu\mu}R_\mu^*\right)\sigma_{\mu\nu}\gamma_{\mu\nu}S_\nu^*.
% \end{equation}
% Also,
% \begin{equation}
% \sum_\nu \abs{B_{i\nu}} = \sum_\nu \sigma_{i\nu}\gamma_{i\nu} S_i^* = \sigma_{\nu\nu}\gamma_{\nu\nu}S^*_\nu,
% \end{equation}
% and
% \begin{equation}
% \sum_j \abs{\Gamma_{\mu j}}= \abs{\Gamma_{\mu\mu}}=\abs{\alpha_{\mu\mu}-\gamma_{\mu\mu}R^*_\mu}\sigma_{\mu\mu}\gamma_{\mu\mu}S^*_\mu,
% \end{equation}
% such that
% \begin{equation}
% R_C = \max\left(\max_\mu\left\{\sigma_{\mu\mu}\gamma_{\mu\mu}S^*_\mu\right\}, \max_\mu \left\{\abs{\alpha_{\mu\mu}-\gamma_{\mu\mu}R^*_\mu}\sigma_{\mu\mu}\gamma_{\mu\mu}S^*_\mu\right\}\right).
% \end{equation}
% Then we have clearly:
% \begin{empheq}{align}
% (\Gamma\Beta)_{\mu\mu} = \left(\alpha_{\mu\mu}-\gamma_{\mu\mu}R_\mu^*\right)\sigma_{\mu\mu}\gamma_{\mu\mu}S_\mu^*
% \end{empheq}

\subsection{Flux analysis - a way to get a sense of scales}
A natural scale free order parameter that at first sight controls the behaviour of the system is the ratio of the syntrophy and consumption fluxes.

The rate of consumption (or \textit{consumption flux}) of species $i$ is given by $\sum_\nu \sigma_{i\nu}\gamma_{i\nu}R_\nu S_i$. Hence the total consumption flux $C_{\text{tot}}$ is given by:
\begin{equation}
C_{\text{tot}} = \sum_{i, \nu} \sigma_{i\nu} \gamma_{i\nu}R_\nu S_i.
\end{equation}
We can similarly define the total syntrophy flux of the system $S_{\text{tot}}$:
\begin{equation}
S_{\text{tot}} = \sum_{i, \nu} \alpha_{\nu i} S_i.
\end{equation}
A natural order parameter $O$ is then
\begin{equation}
O \equiv \frac{S_{\text{tot}}}{C_{\text{tot}}} = \frac{\sum_{i, \nu} \alpha_{\nu i} S_i}{\sum_{i,\nu}\sigma_{i\nu} \gamma_{i\nu}R_\nu S_i} \approx \frac{N_S N_R \alpha_0 S_0}{\sigma_0 R_0 S_0 N_S N_R \gamma_0} = \frac{\alpha_0}{\sigma_0 R_0 \gamma_0}.
\end{equation}

\subsection{Application: fully connected consumption and syntrophy networks}
\input{fully_connected_network}



\end{document}
