\documentclass[12pt, titlepage]{report}
\usepackage{consumer_resource_final}
\graphicspath{{./figures/}}

\begin{document}
As an application, we consider the very special case of the fully connected consumption and syntrophy networks. In this ``mean-field'' theory, every consumer consumes and releases each resource, \ie
\begin{equation}
G_{i\mu}=A_{\mu i}=1 \ \forall \mu=1, \dots, N_R \ \ \forall i=1, \dots, N_S.
\end{equation}
Our goal is to find the spectrum of systems with such consumption and syntrophy networks.
\citeauthor{barbier_cavity_2017} showed that the variance of the interaction matrix plays a leading role in the dynamics of their model \cite{barbier_cavity_2017}. We follow an approach similar to theirs and perform a \important{standard deviation expansion}.
\paragraph{Standard deviation expansion}
The idea behind the standard deviation expansion is the following. Let $q_{i\mu}$ be an arbitrary matrix of size $N_S \times N_R$. The nice trick done in \cite{barbier_cavity_2017} is to write the elements of the $q$ matrix in terms of new variables $\tilde{q}_{i\mu}$:
\begin{equation}
q_{i\mu} = \av{q} + \sigma_q \tilde{q}_{i\mu}. \label{eq : separation matrix average standard dev}
\end{equation}
In that expression, $\av{q}$ is the average of $q$, element-wise
\begin{equation}
\av{q}\defined \frac{\sum_{\mu,i} q_{i\mu}}{N_S N_R},
\end{equation}
and $\sigma_q$ is the standard deviation of $q$, again element-wise:
\begin{equation}
\sigma_q \defined \sqrt{\av{q^2}-\av{q}^2} \text{ with } \av{q^2}\defined \frac{\sum_{\mu,i} q_{i\mu}^2}{N_S N_R}.
\end{equation}
The main advantage of this procedure is that we get a clear idea about the scales involved. A matrix element $q_{i\mu}$ is roughly the mean $\av{q}$ plus a deviation $\sigma_q$ multiplied by a factor of magnitude $\sim$ 1. Indeed the $\tilde{q}_{i\mu}$ are not large since they follow the two equalities \cite{barbier_cavity_2017}:
\begin{equation}
\av{\tilde{q}} = 0 \text{ and } \av{\tilde{q}^2} = 1.
\end{equation}
We apply this framework to our problem by noticing that if the $q_{i\mu}$ are all random samples coming from the same distribution law $\mathfrak{Q}$, we can write the following approximation in the case $N_R, N_S \gg 1 $:
\begin{equation}
\av{q} \approx \av{\mathfrak{Q}} \defined q_0.
\end{equation}
We can then rewrite the free parameters of our model\footnote{This works with $\gamma$ and $\alpha$ because $G$ and $A$ have a trivial topology. Otherwise we would have to take their structure into account and the computations would not be as easy.}:
\begin{subequations}\label{eq : rewrite with metaparameters}
\begin{empheq}[left=\empheqlbrace]{align}
l_\nu & \approx l_0 + \sigma_l \tilde{l}_\nu \\
R^*_\nu & \approx R_0 + \sigma_R \tilde{r}_\nu \\
S^*_i & \approx S_0 + \sigma_S \tilde{s}_i \\
\gamma_{i\nu} & \approx \gamma_0 + \sigma_\gamma  \tilde{g}_{i\nu} \\
\alpha_{\nu i} & \approx \alpha_0 + \sigma_\alpha \tilde{\alpha}_{\nu i} \\
\sigma_{i\nu } & \approx \sigma_0 + \sigma_\sigma \tilde{\sigma}_{i\nu}
\end{empheq}
\end{subequations}
	 The strategy is to assume that the standard deviations are small which allows us to proceed to a first order Taylor expansion.
	 \paragraph{Rewriting the jacobian at equilibrium}
	 The different blocks of the jacobian at equilibrium \eqref{eq: jacobian at equilibrium} can be written with the new variables :
	 \begin{empheq}[left=\empheqlbrace]{align}
	 \frac{l_\mu + \sum_j \alpha_{\mu j} S^*_j}{R^*_\mu} &= \frac{l_0 + N_S \alpha_0 S_0 + \sigma_l \tilde{l}_\mu + \sigma_\alpha S_0 \sum_j \tilde{\alpha}_{\mu j} + \sigma_\alpha \sigma_S \sum_j \tilde{\alpha}_{\mu j} \tilde{s}_j}{R_0 + \sigma_R \tilde{r}_\mu} \\
	 -\gamma_{j\mu}R^*_\mu + \alpha_{\mu j} &= -\gamma_0 R_0 + \alpha_0 + \sigma_\gamma R_0 \tilde{\gamma}_{j\mu} + \sigma_R \gamma_0 \tilde{r}_\mu + \sigma_\alpha \tilde{\alpha}_{\mu j} + \sigma_\gamma \sigma_R \tilde{\gamma}_{j\mu} \tilde{r}_\mu \\
	 \sigma_{i\nu}\gamma_{i\nu}S^*_i &= \sigma_0 \gamma_0 S_0 + \sigma_\sigma \gamma_0 S_0 \tilde{\sigma}_{i\nu} + \sigma_{\gamma} \sigma_0 S_0 \tilde{\gamma}_{i\nu}+\sigma_S \sigma_0 \gamma_0 \tilde{s}_i + \sigma_\sigma \sigma_\gamma S_0 \tilde{\sigma}_{i\nu}\tilde{\gamma}_{i\nu} \nonumber \\
	  & \ \ \ + \sigma_\sigma \sigma_S \gamma_0 \tilde{\sigma}_{i\nu} \tilde{s}_i + \sigma_\gamma \sigma_S \sigma_0 \tilde{\gamma}_{i\nu}\tilde{s}_i + \sigma_{\sigma} \sigma_{\gamma} \sigma_{S} \tilde{\sigma}_{i\nu}\tilde{\gamma}_{i\nu}\tilde{s}_i
	 \end{empheq}
	 It's easier to work with relative standard deviations, \ie we rewrite for all parameters :
	 \begin{equation}
	 \sigma_P \equiv \epsilon_P \av{P}, \ \forall P \in\{l_\nu, R^*_\nu, S^*_i, \gamma_{i\nu}, \alpha_{\nu i}, \sigma_{i\nu}\}.
	 \end{equation}
	 The previous relations then become :
	 \begin{empheq}[left=\empheqlbrace]{align}
	 \frac{l_\mu + \sum_j \alpha_{\mu j} S^*_j}{R^*_\mu} &= \frac{l_0 \left(1+\epsilon_l \tilde{l}_\mu\right)+ \alpha_0 S_0 \left( N_S +\epsilon_\alpha \sum_j \tilde{\alpha}_{\mu j} + \epsilon_\alpha \epsilon_S \sum_j \tilde{\alpha}_{\mu j} \tilde{s}_j \right) }{R_0 \left(1+\epsilon_R\tilde{r}_\mu\right)} \\
	 -\gamma_{j\mu}R^*_\mu + \alpha_{\mu j} &= -\gamma_0 R_0 \left(1+\epsilon_\gamma \tilde{\gamma}_{j\mu} + \epsilon_R  \tilde{r}_\mu + \epsilon_\gamma \epsilon_R  \tilde{\gamma}_{j\mu} \tilde{r}_\mu\right) + \alpha_0 \left(1+\epsilon_\alpha \tilde{\alpha}_{\mu j}  \right) \\
	 \sigma_{i\nu}\gamma_{i\nu}S^*_i &= \sigma_0 \gamma_0 S_0 \left( 1 + \epsilon_\sigma \tilde{\sigma}_{i\nu} + \epsilon_{\gamma}  \tilde{\gamma}_{i\nu}+\epsilon_S  \tilde{s}_i + \epsilon_\sigma \epsilon_\gamma  \tilde{\sigma}_{i\nu}\tilde{\gamma}_{i\nu}  +\epsilon_\sigma \epsilon_S  \tilde{\sigma}_{i\nu} \tilde{s}_i  \right. \nonumber \\
	  & \ \ \  \left. + \epsilon_\gamma \epsilon_S  \tilde{\gamma}_{i\nu}\tilde{s}_i+ \epsilon_{\sigma} \epsilon_{\gamma} \epsilon_{S}  \tilde{\sigma}_{i\nu}\tilde{\gamma}_{i\nu}\tilde{s}_i \right)
	 \end{empheq}


	 \subsubsection{Standard deviation expansion at first order}
	 We assume the relative standard deviations are small:
	 \begin{equation}
	 \epsilon_P \ll 1, \ \forall P \in\{l_\nu, R^*_\nu, S^*_i, \gamma_{i\nu}, \alpha_{\nu i}, \sigma_{i\nu}\}.
	 \end{equation}
	 The previous equations can be rewritten as:
	 \begin{empheq}[left=\empheqlbrace]{align}
	 \frac{l_\mu + \sum_j \alpha_{\mu j} S^*_j}{R^*_\mu} &= \frac{l_0+N_S\alpha_0 S_0}{R_0} - \epsilon_R \frac{l_0+N_S\alpha_0 S_0}{R_0} \tilde{r}_\mu + \epsilon_l \frac{l_0}{R_0} \tilde{l}_\mu + \epsilon_\alpha \frac{\alpha_0 S_0}{R_0} \sum_j \tilde{\alpha}_{\mu j} \\
	 -\gamma_{j\mu}R^*_\mu + \alpha_{\mu j} &=  -\gamma_0 R_0 + \alpha_0 - \epsilon_\gamma \gamma_0 R_0 \tilde{\gamma}_{j \mu} - \epsilon_R \gamma_0 R_0 \tilde{r}_\mu +\epsilon_\alpha \alpha_0 \tilde{\alpha}_{\mu j}\\
	 \sigma_{i\nu}\gamma_{i\nu}S^*_i &=\sigma_0 \gamma_0 S_0 \left( 1 + \epsilon_\sigma \tilde{\sigma}_{i\nu} + \epsilon_{\gamma}  \tilde{\gamma}_{i\nu}+\epsilon_S  \tilde{s}_i \right)
	 \end{empheq}
	 where we neglect all terms of order $\bigO(\epsilon^2)$.
	 We now assume furthermore that the relative standard deviations of every parameter in the model more or less have the same value which again is assumed small, \ie we set :
	 \begin{equation}
	 \epsilon_P \approx \epsilon \ll 1, \ \forall P \in\{l_\nu, R^*_\nu, S^*_i, \gamma_{i\nu}, \alpha_{\nu i}, \sigma_{i\nu}\}.
	 \end{equation}
	 This allows us to rewrite the jacobian at equilibrium as:
	 \begin{equation}
	 J^* = J_0 + \epsilon \tilde{J},
	 \end{equation}
	 with
	 \begin{equation}
	 J_0 = \begin{pmatrix}
	 -\Delta_0 & \Gamma_0  \\
	 B_0 & 0
	 \end{pmatrix}
	 \end{equation}
	 where
	 \begin{empheq}{align}
	 (\Delta_0)_{\mu \nu} &\defined \Delta_0 \delta_{\mu \nu } = \frac{l_0 + N_S \alpha_0 S_0}{R_0} \delta_{\mu \nu} \\
	 (\Gamma_0)_{\mu i} & \defined \Gamma_0 = -\gamma_0 R_0 + \alpha_0 \\
	 (\Beta_0)_{i \nu} & \defined \Beta_0 = \sigma_0 \gamma_0 S_0
	 \end{empheq}
	 and
	 \begin{equation}
	 \tilde{J} = \begin{pmatrix}
	 \tilde{\Delta} & \tilde{\Gamma} \\
	 \tilde{\Beta} & 0
	 \end{pmatrix}
	 \end{equation}
	 with
	 \begin{empheq}{align}
	 \tilde{\Delta}_{\mu \nu} & \defined \left(-\Delta_0 \tilde{r}_\mu + \frac{l_0}{R_0}\tilde{l}_\mu+ \frac{\alpha_0 S_0}{R_0} \sum_j \tilde{\alpha}_{\mu j}\right)\delta_{\mu \nu} \\
	 \tilde{\Gamma}_{\mu i} & \defined \alpha_0 \tilde{\alpha}_{\mu i} - \gamma_0 R_0 \left(\tilde{\gamma}_{i\mu}+\tilde{r}_\mu\right) \\
	 \tilde{B}_{i \mu} & \defined B_0 \left(\tilde{\sigma}_{i\mu}+\tilde{\gamma}_{i\mu}+\tilde{s}_i\right)
	 \end{empheq}
	Using Jacobi's formula \cite{magnus_matrix_2019}, the equation $\det\left(J^*-\lambda \identity{N_R+N_S}\right)=0$ can be rewritten as:
	\begin{equation}
	\boxed{
	\det(J_0-\lambda \identity{N_R+N_S} + \epsilon J^*)=\det(J_0-\lambda \identity{N_R+N_S})+\epsilon \ \trace{\text{adj}(J_0-\lambda \identity{N_R+N_S})\tilde{J}}=0.
	}\label{eq : determinant variance first order}
	\end{equation}
	where $\text{adj}\left(\dots\right)$ is the adjugate operator (\ie which yields the transpose of the cofactor matrix). This equation is a complicated polynomial of degree $N_R+N_S$. We do not know how to find an easily computable solution for $\epsilon > 0$. An explicit solution can however be computed when $\epsilon = 0$.

	\paragraph{Zero variance case}\label{sec : methods dynamical stability fully connected zero variance}
	When $\epsilon=0$, Eq.\eqref{eq : determinant variance first order} becomes:
	\begin{equation}
	\det(J_0-\lambda \identity{N_R+N_S})=\det\begin{pmatrix}
	-\Delta_0-\lambda \identity{N_R} & \Gamma_0 \\
	B_0 & -\lambda \identity{N_S}
	\end{pmatrix}
	=
	0
	\end{equation}
	If we assume that $\lambda\neq 0$, using a reasoning similar to Section \ref{section: non marginal equilibria} we can write the previous equation as:
	\begin{equation}
	\det\left(\lambda^2 \identity{N_R}+\Delta_0 \lambda - \Gamma_0 \Beta_0 \right)=0 \label{eq : determinant no variance}
	\end{equation}
	Component-wise, we have:
	\begin{equation}
	\left(\lambda^2 \identity{N_R}+\Delta_0 \lambda - \Gamma_0 \Beta_0 \right)_{\mu\nu} = \left(\lambda^2+\Delta_0 \lambda\right)\delta_{\mu \nu} - \Gamma_0\Beta_0.
	\end{equation}
	Appendix \ref{app : special determinant computation} explains how to find the non-zero solutions of Eq.\eqref{eq : determinant no variance}. They are given by the roots of the polynomial:
	\begin{equation}
	\left(\lambda+\Delta_0\right)^{N_R-1}\left(\lambda^2+\Delta_0 \lambda -N_R \Gamma_0 \Beta_0\right)=0.
	\end{equation}
	That equation gives us $N_R-1+2=N_R+1$ non-zero eigenvalues, which means that there are $N_S-1$ zero eigenvalues. The two eigenvalues different from $-\Delta_0$ or $0$ are the roots of the second degree polynomial:
	\begin{equation}
	\lambda^2 + \Delta_0 \lambda- N_R \Gamma_0 \Beta_0 =0.
	\end{equation}
	In the end, the spectrum is given by:
	\begin{itemize}
	\item if $\Gamma_0 < -\frac{\Delta_0^2}{4 N_R \Beta_0}$ :
	\begin{equation}
	\sigma(J_0)= \left\{0, \dots, 0, -\Delta_0, \dots, -\Delta_0, -\frac{\Delta_0}{2} \left(1\pm i \sqrt{-\left(1+\frac{4N_R\Gamma_0\Beta_0}{\Delta_0^2}\right)}\right)\right\}
	\end{equation}
	\item if $\Gamma_0 = -\frac{\Delta_0^2}{4 N_R \Beta_0}$ :
	\begin{equation}
	\sigma(J_0)= \left\{0, \dots, 0, -\Delta_0, \dots, -\Delta_0, -\frac{\Delta_0}{2}, -\frac{\Delta_0}{2}\right\}
	\end{equation}
	\item if $\Gamma_0 > -\frac{\Delta_0^2}{4 N_R \Beta_0}$ :
	\begin{equation}
	\sigma(J_0)= \left\{0, \dots, 0, -\Delta_0, \dots, -\Delta_0, -\frac{\Delta_0}{2} \left(1\pm \sqrt{1+\frac{4N_R\Gamma_0\Beta_0}{\Delta_0^2}}\right)\right\} \label{eq : spectrum fully connected possibly negative}
	\end{equation}
	\end{itemize}
	It then becomes clear that the system is dynamically unstable if and only if $\frac{4N_R\Gamma_0 \Beta_0}{\Delta_0^2} > 0 $. Because $N_R, \Beta_0 > 0$, we get the condition:
	\begin{equation}
	\boxed{
	\text{The non-variance system is dynamically unstable} \iff \Gamma_0 > 0.
	}
	\end{equation}
	If $\Gamma_0 \leq 0$, the fully connected system will be marginally stable.
	Note that the RHS of the feasibility condition Eq.\eqref{eq : fully feasible volume} is equivalent in the fully connected case to :
	\begin{equation}
	\alpha_0 \lessapprox \min(1-\sigma_0, \sigma_0)\gamma_0 R_0 \iff \Gamma_0 \lessapprox \left[\min(1-\sigma_0, \sigma_0)-1\right] \gamma_0 R_0 < 0,
	\end{equation}
	which means that the case $\Gamma_0 > 0$ is simply not feasible. So in the end the feasible fully connected case is marginally stable and its non-zero eigenvalues have a negative real part.

	We see that all but one non-zero eigenvalues are given by $-\Delta_0$. However the last eigenvalue is determined by
	$4N_R\Gamma_0\Beta_0+\Delta_0^2$. It gives us the ``deviation'' away from $-\Delta_0/2$ and hence also plays an essential role in the stability. Looking at its sign allows us to find possibly more locally dynamically stable zones of the metaparameters space $\mathcal{M}$. Indeed we expect to find more stable systems in the case of Eq.\eqref{eq : spectrum fully connected possibly negative} if $4 N_R \Gamma_0 \Beta_0 + \Delta_0 ^2$ is small, \ie
	\begin{equation}
	4 N_R \Gamma_0 \Beta_0 + \Delta_0 ^2 \ll 1 \iff 4N_R \sigma_0 \gamma_0 S_0  (\alpha_0 - \gamma_0 R_0) +\frac{l_0^2}{R_0^2}+\frac{2N_S\alpha_0 S_0 l_0}{R_0^2} + \frac{N_S^2 \alpha_0^2 S_0^2}{R_0^2} \ll 1. \label{eq : dynamical stability methods fully connected metaparameters}
	\end{equation}
	This equation will be useful in order to predict which metaparameters will lead to dynamically stable systems (see Section \ref{sec: largest eigenvalue of the jacobian}).



\end{document}
